{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./aclImdb/\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "test_dir = os.path.join(data_dir, \"test\")\n",
    "TRAIN_SIZE = 20000\n",
    "VALIDATION_SIZE = 5000\n",
    "TEST_SIZE = 25000\n",
    "\n",
    "def read_file(file_name):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        content = f.read()\n",
    "        content = content.lower().replace(\"<br />\", \"\")\n",
    "    return content\n",
    "\n",
    "def load_dataset(dataset_dir, dataset_size, initial=0):\n",
    "    pos_dir = os.path.join(dataset_dir, \"pos\")\n",
    "    neg_dir = os.path.join(dataset_dir, \"neg\")\n",
    "    single_label_size = int(dataset_size / 2)\n",
    "    dataset = []\n",
    "    target = []\n",
    "    all_pos = os.listdir(pos_dir)\n",
    "    all_neg = os.listdir(neg_dir)\n",
    "    for i in range(initial, initial+single_label_size):\n",
    "        dataset.append(read_file(os.path.join(pos_dir, all_pos[i])))\n",
    "        target.append(1)\n",
    "        dataset.append(read_file(os.path.join(neg_dir, all_neg[i])))\n",
    "        target.append(0)\n",
    "    return dataset, target\n",
    "\n",
    "train_data = load_dataset(train_dir, TRAIN_SIZE)[0]\n",
    "train_targets = load_dataset(train_dir, TRAIN_SIZE)[1]\n",
    "validation_data = load_dataset(train_dir, VALIDATION_SIZE, initial=int(TRAIN_SIZE/2))[0]\n",
    "validation_targets = load_dataset(train_dir, VALIDATION_SIZE, initial=int(TRAIN_SIZE/2))[1]\n",
    "test_data = load_dataset(test_dir, TEST_SIZE)[0]\n",
    "test_targets = load_dataset(test_dir, TEST_SIZE)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/JaneYY/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "punctuations = string.punctuation\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "def tokenize(sent):\n",
    "#     token_list = []\n",
    "    tokens = word_tokenize(sent)\n",
    "    for token in tokens:\n",
    "        token_list = [w for w in tokens if not w in stop_words]\n",
    "    return [token.lower() for token in token_list if (token not in punctuations)]\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n",
      "Tokenizing validation data\n",
      "Tokenizing test data\n"
     ]
    }
   ],
   "source": [
    "def find_ngrams(input_list, n): #input_list is a list of single tokens\n",
    "    result = []\n",
    "    result += list(zip(*[input_list[j:] for j in range(n)]))\n",
    "    return result\n",
    "\n",
    "def tokenize_dataset(dataset, n):\n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        ngrams = find_ngrams(tokens,n)\n",
    "        new_tokens = [\" \".join(list(i)) for i in ngrams]\n",
    "        token_dataset.append(new_tokens)\n",
    "        all_tokens += new_tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "n = 1\n",
    "\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(train_data,n)\n",
    "pkl.dump(all_train_tokens, open(\"stop_all_train_tokens_\" + str(n) + \".p\", \"wb\"))\n",
    "pkl.dump(train_data_tokens, open(\"stop_train_data_tokens_\" + str(n) + \".p\", \"wb\"))\n",
    "\n",
    "print (\"Tokenizing validation data\")\n",
    "validation_data_tokens, _ = tokenize_dataset(validation_data,n)\n",
    "pkl.dump(validation_data_tokens, open(\"stop_validation_data_tokens_\" + str(n) + \".p\", \"wb\"))\n",
    "\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_data,n)\n",
    "pkl.dump(test_data_tokens, open(\"stop_test_data_tokens_\" + str(n) + \".p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "emb_dim = 100 # dimension for n-gram embedding\n",
    "learning_rate = 0.01\n",
    "num_epochs = 3 # number of epochs to train\n",
    "max_vocab_size = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Validation dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "validation_data_indices = token2index_dataset(validation_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Validation dataset size is {}\".format(len(validation_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of IMDB tokens \n",
    "        @param target_list: list of IMDB targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "    \n",
    "MAX_SENTENCE_LENGTH = 250   \n",
    "    \n",
    "def IMDB_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "train_dataset = IMDBDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = IMDBDataset(validation_data_indices, validation_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = IMDBDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfNGram(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfNGram classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfNGram, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,2)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "model = BagOfNGram(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/625], Validation Acc: 84.04\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 86.48\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 86.78\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 87.66\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 88.42\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 88.68\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 88.78\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 88.88\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 89.14\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 89.04\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 89.06\n",
      "early stop triggered\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def early_stop(val_acc_history, t=2, required_progress=0.005):\n",
    "    \"\"\"\n",
    "    Stop the training if there is no non-trivial progress in k steps\n",
    "    @param val_acc_history: a list contains all the historical validation acc\n",
    "    @param required_progress: the next acc should be higher than the previous by \n",
    "        at least required_progress amount to be non-trivial\n",
    "    @param t: number of training steps \n",
    "    @return: a boolean indicates if the model should earily stop\n",
    "    \"\"\"\n",
    "    # TODO: add your code here\n",
    "    if len(val_acc_history) >=t+1:\n",
    "        if val_acc_history[-1] - val_acc_history[-1-t] <= required_progress:\n",
    "            return True\n",
    "\n",
    "\n",
    "validation_acc_history = []\n",
    "stop_training = False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "            validation_acc_history.append(val_acc)\n",
    "            stop_training = early_stop(validation_acc_history)\n",
    "            if stop_training:\n",
    "                print(\"early stop triggered\")\n",
    "                break\n",
    "    if stop_training:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc 87.624\n"
     ]
    }
   ],
   "source": [
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:tensor([[ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 0],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0]])\n",
      "label:tensor([[ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0]])\n",
      "predicted:tensor([[ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0]])\n",
      "label:tensor([[ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0]])\n",
      "predicted:tensor([[ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0]])\n",
      "label:tensor([[ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 0]])\n"
     ]
    }
   ],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    i = 0\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "        print('predicted:{}'.format(predicted))\n",
    "        print('label:{}'.format(labels.view_as(predicted)))\n",
    "        i +=1\n",
    "        if i >=3:\n",
    "            break\n",
    "    return (100 * correct / total)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "val_acc = test_model(val_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: there are enough sad stories about women and their oppression by religious, political and societal means. not to diminish the films and stories about genital mutilation and reproductive rights, as well as wage inequality, and marginalization in society, all in the name of allah or god or some other ridiculous justification, but sometimes it is helpful to just take another approach and shed some light on the subject.the setting is the 2006 match between iran and bahrain to qualify for the world cup. passions are high and several women try to disguise themselves as men to get into the match.the women who were caught (played by sima mobarak-shahi, shayesteh irani, ayda sadeqi, golnaz farmani, and mahnaz zabihi) and detained for prosecution provided a funny and illuminating glimpse into the customs of this country and, most likely, all muslim countries. their interaction with the iranian soldiers who were guarding and transporting them, both city and villagers, and the father who was looking for his daughter provided some hilarious moments as we thought about why they have such unwritten rules.it is mainly about a paternalistic society that feels it has to save it's women from the crude behavior of it's men. rather than educating the male population, they deny privilege and rights to the women.seeing the changes in the soldiers responsible and the reflection of iranian society, it is nos surprise this film will not get any play in iran. but jafar panahi has a winner on his hands for those able to see it.\n",
      "Example 2: a group of extremely unlikable a-holes are tormented by lame puppets that some elderly douche bag night-watchman has kept locked away in a film vault for twenty years for no reason whatsoever.many people know this film merely from mst3k's spot-on ribbing of the flick. but i've seen the actual movie and can safely say that yes it's bad, really, really bad. from the one of the most awful 'fight' scenes i've ever witnessed to the stuffed toy 'aliens' that suffer from a lack of motion (i had a my pet monster that was scarier) right up to the atrocious acting (i had a my pet monster that was more charismatic) however, that being said rick sloan's \"vice academy\" films are somehow, and trust me i have no earthly idea how, much worse. that's not to suggest that this film is anything but crap, because it isn't. just throwing it out there.eye candy: no nudity in the movie proper, but there's 2 pairs of tits in the dvd introduction to the film my grade: d- retromedia dvd extras: introduction by jim wynorski; stills gallery; and trailer for this film\n",
      "Example 3: \"moonstruck\" is a lovely little film directed by superb story teller, norman jewison (in the heat of the night, fiddler on the roof, the hurricane). the film is great on many levels. it shows a good slice of italian culture, has a touching romance, and (best of all) is a hilarious comedy.one thing i liked most about the film was the relative unconventional looks of the actors. nicolas cage looks positively odd for most of the film, and cher... well, cher always looks a little odd.overall, it's a fun film, and easy to recommend.7.4 out of 10\n"
     ]
    }
   ],
   "source": [
    "print('Example 1: ' + validation_data[0])\n",
    "print('Example 2: ' + validation_data[1])\n",
    "print('Example 3: ' + validation_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: twisted desire (1996) was a tv movie starring melissa joan hart. melissa's character, jennifer stanton, a seventeen-year-old seduces her current boyfriend nick ryan into murdering her two parents. the movie is based on the 1990 murders of the parents of 14 year old jessica wiseman. jessica had her 17 year old boyfriend douglas christopher thomas shoot and kill her parents! thomas was executed in 2000! jessica was released from prison when she turned 21 years old. evidence now suggests that it was jessica who fired the fatal shot that killed her mother. jessica is known to now be residing somewhere in the state of virginia.\n",
      "Example 2: this movie's one of my favorites. it's not really any good, but it's great to laugh at. the dialogue can become incredibly ludicrous and poorly acted (eg, \"manji, can we ask you a few questions?\" \"sure.\" \"we think you can help us with the answers.\") any fighting is more or less surrealistic. make sure to watch for brock, the oafy white guy who attacks the main characters. he only has two lines, but he's one of the best guys in the movie!\n",
      "Example 3: well, if it weren't for ethel waters and a 7-year-old sammy davis, jr. (here billed without the jr.), rufus jones for president would be one of the worst representations of african-american stereotypes i've seen from the early talkie era and wouldn't have been worth seeing because of that. ms. waters is excellent here singing \"am i blue?\" and \"underneath our harlem moon\" while mr. davis shows us how his childhood experience in showbiz prepared him for his superstar status as an adult. he's so good tap-dancing here that for awhile i thought he was a little person with decades of experience. so if you're willing to ignore the negative connotations here, rufus jones for president should provide some good enjoyment. p.s. this marks the fourth time today i've seen and heard the song, \"i'll be glad when you're dead you rascal you\" performed on film, this time by davis. must have been a very popular song about this time.\n"
     ]
    }
   ],
   "source": [
    "print('Example 1: ' + validation_data[8])\n",
    "print('Example 2: ' + validation_data[51])\n",
    "print('Example 3: ' + validation_data[75])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
